# Collect list of book names and ids 

# getting the libraries we need
import requests
from bs4 import BeautifulSoup as bs
from random import randint
from time import sleep

# define file to store the output
file = open("file.txt","a")

# input url to be scraped - remove page number if scraping multiple URLs from the same website
URL = 'https://www.goodreads.com/list/show/2.The_Worst_Books_of_All_Time?page='

# worst books list has 77 pages -  iterate through each one of them
for page in range(1,77): 
    # control crawl rate by adding random pauses or little breaks 
    sleep(randint(4,7))
    # request data from the server
    req = requests.get(URL + str(page))
    print(URL + str(page))
    # parse data 
    soup = bs(req.text, 'html.parser')
    
    #get <a> tag, href inside it and extract book id and book title from link
    for link in soup.find_all('a'):
        book = str(link.get('href'))
        if "/book/show/" in book:
            split = book.split('/')
            title = split[3]
            # avoid storing duplicates
            if (a is None) or (title!=a):
                print(title)
                # write file and add break line after each book
                file.write(title+'\n')
                a=title
# close file                
file.close()

Using the resulted list - followed steps in this github repo to scrape the web - https://github.com/maria-antoniak/goodreads-scraper 
